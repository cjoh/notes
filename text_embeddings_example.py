import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Example: Simple text embeddings (simplified for demonstration)
# In reality, these would be generated by models like Word2Vec, BERT, or OpenAI's embeddings

# Let's say we have embeddings for these sentences (3-dimensional vectors for simplicity)
embeddings = {
    "I love programming": np.array([0.8, 0.2, 0.5]),
    "I enjoy coding": np.array([0.7, 0.3, 0.6]),
    "I like to write software": np.array([0.75, 0.25, 0.55]),
    "I hate bugs": np.array([0.3, -0.5, 0.4]),
    "The weather is nice": np.array([-0.2, 0.8, -0.3])
}

# Calculate similarities between sentences
print("Text Embedding Similarities:\n")
sentences = list(embeddings.keys())

for i, sent1 in enumerate(sentences):
    for j, sent2 in enumerate(sentences):
        if i < j:  # Only calculate upper triangle
            similarity = cosine_similarity(
                embeddings[sent1].reshape(1, -1),
                embeddings[sent2].reshape(1, -1)
            )[0][0]
            print(f'"{sent1}" vs "{sent2}"')
            print(f"Similarity: {similarity:.3f}\n")

# Demonstrate how embeddings capture meaning
print("\nKey Insights:")
print("- Similar sentences have high cosine similarity (close to 1)")
print("- Dissimilar sentences have low similarity (close to 0 or negative)")
print("- The vectors encode semantic relationships between texts")